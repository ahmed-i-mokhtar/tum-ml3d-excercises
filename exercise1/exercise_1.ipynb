{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Exercise Information\n",
    "Welcome to the exercise for **Machine Learning for 3D Geometry**! ðŸ™‚\n",
    "\n",
    "Some administrative things before you begin with the first exercise:\n",
    "\n",
    "- You have to submit and pass all 3 exercises to get the 0.3 exam bonus\n",
    "- You can either work on an exercise alone or team up with one other student; if you work in a team, only one of you has to hand in your solution but make sure to name the submission correctly (see next bullet point)\n",
    "- Name your submission as your matriculation number(s) separated by underscores, e.g. \"0123456_0123457.zip\"\n",
    "- To successfully complete an exercise, fill out the blanks in this jupyter notebook and all referenced files. Make sure all cells in this notebook run as expected. Then, submit everything, including files you generated during the exercise in a single zip file via the submission feature in Moodle\n",
    "- For each exercise, you have two weeks of working time. Note the deadline date on top of each notebook\n",
    "- We will upload and present the solutions to each exercise after the respective deadline\n",
    "- If you have questions, feel free to ask them in the Moodle forum or directly contact us via email at can.guemeli@tum.de, jiapeng.tang@tum.de, or daoyi.gao@tum.de"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: Surface Representations and Shape Alignment\n",
    "\n",
    "**Submission Deadline**: 22.11.2023 23:55\n",
    "\n",
    "In this exercise, we will go over a few basic geometric tasks including hand-crafted shape generation and conversion between representations as well as simple shape alignment. It is fundamental to understand how these work before we dive into various machine learning tasks in the next two exercises. Being able to solve these tasks will also make working on your projects a lot easier later on.\n",
    "\n",
    "First of all, make sure you installed all necessary requirements. They are specified in `pyproject.toml`, an easy way to install all of them in a new virtual environment is to use [poetry](https://python-poetry.org), have it install everything (`poetry install`) and run this notebook via `poetry run jupyter notebook`.\n",
    "\n",
    "The following imports should then work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Installing C:\\Users\\zeyad.mahmoud\\anaconda3\\Lib\\site-packages\\k3d\\static -> k3d\n",
      "Up to date: C:\\Users\\zeyad.mahmoud\\AppData\\Roaming\\jupyter\\nbextensions\\k3d\\extension.js\n",
      "Up to date: C:\\Users\\zeyad.mahmoud\\AppData\\Roaming\\jupyter\\nbextensions\\k3d\\fflate.js\n",
      "Up to date: C:\\Users\\zeyad.mahmoud\\AppData\\Roaming\\jupyter\\nbextensions\\k3d\\headless.html\n",
      "Up to date: C:\\Users\\zeyad.mahmoud\\AppData\\Roaming\\jupyter\\nbextensions\\k3d\\index.js\n",
      "Up to date: C:\\Users\\zeyad.mahmoud\\AppData\\Roaming\\jupyter\\nbextensions\\k3d\\index.js.map\n",
      "Up to date: C:\\Users\\zeyad.mahmoud\\AppData\\Roaming\\jupyter\\nbextensions\\k3d\\require.js\n",
      "Up to date: C:\\Users\\zeyad.mahmoud\\AppData\\Roaming\\jupyter\\nbextensions\\k3d\\snapshot_inline.txt\n",
      "Up to date: C:\\Users\\zeyad.mahmoud\\AppData\\Roaming\\jupyter\\nbextensions\\k3d\\snapshot_online.txt\n",
      "Up to date: C:\\Users\\zeyad.mahmoud\\AppData\\Roaming\\jupyter\\nbextensions\\k3d\\snapshot_standalone.txt\n",
      "Up to date: C:\\Users\\zeyad.mahmoud\\AppData\\Roaming\\jupyter\\nbextensions\\k3d\\standalone.js\n",
      "Up to date: C:\\Users\\zeyad.mahmoud\\AppData\\Roaming\\jupyter\\nbextensions\\k3d\\standalone.js.map\n",
      "- Validating: ok\n",
      "\n",
      "    To initialize this nbextension in the browser every time the notebook (or other app) loads:\n",
      "    \n",
      "          jupyter nbextension enable k3d --user --py\n",
      "    \n",
      "Enabling notebook extension k3d/extension...\n",
      "      - Validating: ok\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "!jupyter nbextension install --py --user k3d\n",
    "!jupyter nbextension enable --py --user k3d\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import k3d\n",
    "import trimesh\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Generating Signed Distance Fields\n",
    "In the lecture, the concept of implicitly encoded shapes was introduced.\n",
    "Here, we are specifically interested in generating a Signed Distance Field representation.\n",
    "\n",
    "In a Signed Distance Field (SDF), the surface of a shape is represented as the zero-level set of a function which maps point locations to their distance from the closest surface.\n",
    "In addition, the sign encodes whether a voxel lies within a shape (negative sign) or outside (positive sign).\n",
    "\n",
    "Let's first take a look at some simple examples of such functions.\n",
    "\n",
    "### (a) SDF for a Sphere\n",
    "The visualization below shows on the right a slice through the SDF representation of the sphere on the left (positive values in blue, negative in red, close to zero as gray).\n",
    "\n",
    "Sphere Mesh | Sphere SDF\n",
    ":- | :-\n",
    "<img src=\"exercise_1/images/sphere_mesh.png\" alt=\"sphere_mesh\" style=\"width: 250px;\"/> | <img src=\"exercise_1/images/sphere_sdf.png\" alt=\"sphere_sdf\" style=\"width: 250px;\"/>\n",
    "\n",
    "In this part of the exercise, we do not yet aim to create a grid of SDF values but instead first focus on the evaluation at sparse 3D point locations.\n",
    "\n",
    "Fill out the function `signed_distance_sphere` in file `exercise_1/implicit_function.py` such that it describes the SDF of a sphere at any given point. Make sure that the results you get in the cell below match the expected results as described in the comments.\n",
    "\n",
    "Hint: Distances to the surface of a sphere centered at (0, 0, 0) with radius **r** can be defined as $$f(x, y, z) = \\sqrt{x^2 + y^2 + z^2} - r$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SDF inside the sphere: -0.3500\n",
      "SDF at surface of the sphere: 0.0000\n",
      "SDF outside the sphere: 0.0500\n",
      "SDF for given points: [-0.35  0.    0.05]\n"
     ]
    }
   ],
   "source": [
    "from exercise_1.implicit_function import signed_distance_sphere\n",
    "\n",
    "sphere_radius = 0.35\n",
    "sphere_center = [0, 0, 0]\n",
    "\n",
    "# sdf value at point [0, 0, 0], i.e. at the center of the sphere should have a negative sdf\n",
    "x, y, z = 0, 0, 0\n",
    "print(f'SDF inside the sphere: {signed_distance_sphere(x, y, z, sphere_radius, sphere_center[0], sphere_center[1], sphere_center[2]):.4f}')\n",
    "\n",
    "# sdf value at point [0, 0, 0.35], i.e. on the surface of the sphere should have sdf value zero\n",
    "x, y, z = 0, 0, 0.35\n",
    "print(f'SDF at surface of the sphere: {signed_distance_sphere(x, y, z, sphere_radius, sphere_center[0], sphere_center[1], sphere_center[2]):.4f}')\n",
    "\n",
    "# sdf value at point [0, 0.4, 0], i.e. outside the sphere should have a positive sdf value\n",
    "x, y, z = 0, 0.4, 0\n",
    "print(f'SDF outside the sphere: {signed_distance_sphere(x, y, z, sphere_radius, sphere_center[0], sphere_center[1], sphere_center[2]):.4f}')\n",
    "\n",
    "# your function should be able to handle numpy arrays as point coordinate inputs\n",
    "# let's pass all three points for vectorize inference\n",
    "x = np.array([0, 0, 0])\n",
    "y = np.array([0, 0, 0.4])\n",
    "z = np.array([0, 0.35, 0])\n",
    "print('SDF for given points:', signed_distance_sphere(x, y, z, sphere_radius, sphere_center[0], sphere_center[1], sphere_center[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) SDF for a Torus\n",
    "Analogous to (a), fill out the function `signed_distance_torus` in file `exercise_1/implicit_function.py` such that the function describes the SDF of a torus.\n",
    "\n",
    "Hint: Distances to the surface of a torus with major radius **R** and minor radius **r** are defined as $$f(x, y, z) = \\sqrt{a^2 + y^2} - r, a=\\sqrt{x^2+z^2}-R$$\n",
    "\n",
    "Torus Mesh | Torus SDF (abs(sdf) > 0.1 not shown)\n",
    ":- | :-\n",
    "<img src=\"exercise_1/images/torus_mesh.png\" alt=\"torus_mesh\" style=\"width: 250px;\"/> | <img src=\"exercise_1/images/torus_sdf.png\" alt=\"torus_sdf\" style=\"width: 250px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SDF for given points: [-3.50000000e-02  2.77555756e-17  3.15000000e-01]\n"
     ]
    }
   ],
   "source": [
    "from exercise_1.implicit_function import signed_distance_torus\n",
    "\n",
    "torus_minor_radius = 0.035\n",
    "torus_major_radius = 0.35\n",
    "torus_center = [0, 0, 0]\n",
    "\n",
    "# let's define a few test points: inside the torus (0, 0, 0.35), surface of the torus (0, 0, 0.385) and outside the torus (0, 0, 0)\n",
    "x = np.array([0, 0, 0])\n",
    "y = np.array([0, 0, 0])\n",
    "z = np.array([0.35, 0.385, 0])\n",
    "print('SDF for given points:', signed_distance_torus(x, y, z, torus_major_radius, torus_minor_radius, torus_center[0], torus_center[1], torus_center[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Composition of SDFs\n",
    "Using (a) and (b), make a function that describes a hydrogen atom like in the visualization below. We'll represent the proton and the electron with spheres, and the orbit of the electron as a thin torus.\n",
    "\n",
    "Fill out the function `signed_distance_atom` in file `exercise_1/implicit_function.py` to realize such a signed distance field function. \n",
    "\n",
    "Hint: We discussed several possible SDF composition operations in the lecture; pick the correct one here.\n",
    "\n",
    "Atom Mesh | Atom SDF (abs(sdf) > 0.1 not shown)\n",
    ":- | :-\n",
    "<img src=\"exercise_1/images/atom_mesh.png\" alt=\"atom_mesh\" style=\"width: 250px;\"/> | <img src=\"exercise_1/images/atom_sdf.png\" alt=\"atom_sdf\" style=\"width: 250px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SDF for given points: [-0.1   -0.01   0.075]\n"
     ]
    }
   ],
   "source": [
    "from exercise_1.implicit_function import signed_distance_atom\n",
    "\n",
    "# size and placement of shapes is specified in the signed_distance_atom function\n",
    "# points x, y, z = (0, 0, 0), (0, 0, 0.35) should both lie inside the composite shape, while (0, 0, 1.75) outside\n",
    "x = np.array([0, 0, 0])\n",
    "y = np.array([0, 0, 0])\n",
    "z = np.array([0, 0.35, 0.175])\n",
    "print('SDF for given points:', signed_distance_atom(x, y, z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d) SDF Grids and Visualization\n",
    "Up until now, you only implemented continuous SDF functions and evaluated them at sparse points.\n",
    "\n",
    "It is usually more practical to represent a shape in a discrete volumetric grid where each voxel contains one SDF value, pointing towards the closest surface. This is what we want to implement here: Use your SDF functions from above to generate a regular voxel grid, then visualize that in 3D.\n",
    "\n",
    "First, fill out the function `sdf_grid` in file `exercise_1/sdf_grid.py` to generate an SDF grid given an SDF function and resolution (number of voxels in each dimension). You can assume a unit cube (side length 1 in each dimension) that is centered at the shape centers defined in (a) and (b).\n",
    "\n",
    "Then, use our pre-defined function `visualize_sdf` in `exercise_1/util/visualization.py` to export your grid as a ply file which you can open in Meshlab to look at and move through your generated SDF.\n",
    "\n",
    "This is also a good time to familiarize yourself with [Meshlab](http://www.meshlab.net/) if you have not done so yet.\n",
    "\n",
    "Our visualization function outputs color-coded cubes for each voxel in the grid: Small blue cubes for positive values (outside the surface) and bigger red cubes for negative values (inside the surface).\n",
    "Cubes very close to the surface are colored in white."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exercise_1.sdf_grid import sdf_grid\n",
    "from exercise_1.util.visualization import visualize_sdf\n",
    "\n",
    "# We use a resolution of 32x32x32 for our grid here. \n",
    "# You can also try out different values but note that higher resolutions take significantly \n",
    "# more time for generating and exporting the grid.\n",
    "resolution = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating SDF visualization for 32^3 grid ...\n",
      "Exported to C:\\Users\\zeyad.mahmoud\\Downloads\\tum-ml3d-excercises\\exercise1\\sphere_sdf.ply\n"
     ]
    }
   ],
   "source": [
    "# Sphere SDF Grid\n",
    "sdf_func_sphere = lambda x, y, z: signed_distance_sphere(x, y, z, sphere_radius, sphere_center[0], sphere_center[1], sphere_center[2])\n",
    "sphere_sdf_grid = sdf_grid(sdf_func_sphere, resolution)\n",
    "visualize_sdf(sphere_sdf_grid, filename=Path.cwd() / 'sphere_sdf.ply')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating SDF visualization for 32^3 grid ...\n",
      "Exported to C:\\Users\\zeyad.mahmoud\\Downloads\\tum-ml3d-excercises\\exercise1\\torus_sdf.ply\n"
     ]
    }
   ],
   "source": [
    "# Torus SDF Grid\n",
    "sdf_func_torus = lambda x, y, z: signed_distance_torus(x, y, z, torus_major_radius, torus_minor_radius, torus_center[0], torus_center[1], torus_center[2])\n",
    "torus_sdf_grid = sdf_grid(sdf_func_torus, resolution)\n",
    "visualize_sdf(torus_sdf_grid, filename=Path.cwd() / 'torus_sdf.ply')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating SDF visualization for 32^3 grid ...\n",
      "Exported to C:\\Users\\zeyad.mahmoud\\Downloads\\tum-ml3d-excercises\\exercise1\\atom_sdf.ply\n"
     ]
    }
   ],
   "source": [
    "# Atom SDF Grid\n",
    "# Here, your might notice that a resolution of 32x32x32 or lower is not sufficient to represent the outer ring of the atom.\n",
    "# This is a common problem especially with machine learning methods that are limited to such a low resolution as we will see in future lectures.\n",
    "\n",
    "atom_sdf_grid = sdf_grid(signed_distance_atom, resolution)\n",
    "visualize_sdf(atom_sdf_grid, filename=Path.cwd() / 'atom_sdf.ply')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. From Signed Distance Fields to Occupancy Grids\n",
    "We can also represent shapes by encoding occupancy instead of SDF values. Used in a grid structure, this leads us to Occupancy Grids which simply encode for each voxel if it is inside or outside the represented shape. This has certain advantages and disadvantages compared to SDF grids, especially for learned methods as we will see in later lectures.\n",
    "\n",
    "### (a) Converting SDF Grids to Occupancy Grids\n",
    "We consider here an Occupancy Grid as a binary 3D grid with value 0 for voxels outside the shape and 1 for voxels inside. Given an SDF of a shape, converting the shape to an occupancy grid is straight forward.\n",
    "\n",
    "Fill out the function `occupancy_grid` in file `exercise_1/occupancy_grid.py` such that the function returns an Occupancy Grid given an existing SDF Grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exercise_1.occupancy_grid import occupancy_grid\n",
    "from exercise_1.util.visualization import visualize_occupancy, visualize_pointcloud, visualize_mesh\n",
    "\n",
    "# Here, we use a grid of 128x128x128\n",
    "resolution = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d940e2ba2294b4f80b049abf5e928e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Sphere Occupancy Grid\n",
    "sphere_occ_grid = occupancy_grid(sdf_func_sphere, resolution)\n",
    "visualize_occupancy(sphere_occ_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4b4edf210cc4c6b9a42672018f53959",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Torus Occupancy Grid\n",
    "torus_occ_grid = occupancy_grid(sdf_func_torus, resolution)\n",
    "visualize_occupancy(torus_occ_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "765c3ac6280a4d20b7b5d5759c9b9ee2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Atom Occupancy Grid\n",
    "atom_occ_grid = occupancy_grid(signed_distance_atom, resolution)\n",
    "visualize_occupancy(atom_occ_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Implicit Functions for more complex shapes\n",
    "\n",
    "The SDF functions you implemented and used so far encode only simple shapes which could be represented by rather straight-forward mathematical functions. What if we need to represent a complex shape, e.g. the stanford bunny? It's hard to come up with an elegant mathematical equation in this case.\n",
    "\n",
    "One way to figure out such implicit shape function is to <em>learn</em> the function in a supervised way. In later lectures, we'll go over works like DeepSDF [1] and Occupancy Networks [2] which are seminal works showing how to represent and learn SDF/Occupancy functions with deep neural networks.\n",
    "\n",
    "For now let's try running your occupancy grid extraction method on a learned function to represent a complex shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pytorch_lightning'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexercise_1\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmlp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m signed_distance_mlp\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# runs on cpu, so might be slow\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m shape_grid \u001b[38;5;241m=\u001b[39m occupancy_grid(signed_distance_mlp, \u001b[38;5;241m63\u001b[39m)\n\u001b[0;32m      5\u001b[0m visualize_occupancy(shape_grid, flip_axes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\Downloads\\tum-ml3d-excercises\\exercise1\\exercise_1\\occupancy_grid.py:19\u001b[0m, in \u001b[0;36moccupancy_grid\u001b[1;34m(sdf_function, resolution)\u001b[0m\n\u001b[0;32m     17\u001b[0m grid \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((resolution, resolution, resolution))\n\u001b[0;32m     18\u001b[0m x, y, z \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmgrid[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m1\u001b[39m:resolution \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1\u001b[39mj, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m1\u001b[39m:resolution \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1\u001b[39mj, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m1\u001b[39m:resolution \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1\u001b[39mj]\n\u001b[1;32m---> 19\u001b[0m grid \u001b[38;5;241m=\u001b[39m sdf_function(x, y, z) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m grid\n",
      "File \u001b[1;32m~\\Downloads\\tum-ml3d-excercises\\exercise1\\exercise_1\\util\\mlp\\model.py:11\u001b[0m, in \u001b[0;36msigned_distance_mlp\u001b[1;34m(x, y, z)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msigned_distance_mlp\u001b[39m(x, y, z):\n\u001b[0;32m     10\u001b[0m     model \u001b[38;5;241m=\u001b[39m SimpleImplicitDecoder()\n\u001b[1;32m---> 11\u001b[0m     state_dict \u001b[38;5;241m=\u001b[39m rename_state_dict(torch\u001b[38;5;241m.\u001b[39mload((pathlib\u001b[38;5;241m.\u001b[39mPath(\u001b[38;5;18m__file__\u001b[39m)\u001b[38;5;241m.\u001b[39mparent \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheckpoint\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_ckpt_epoch=349.ckpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mabsolute(), map_location\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstate_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     12\u001b[0m     model\u001b[38;5;241m.\u001b[39mload_state_dict(state_dict)\n\u001b[0;32m     13\u001b[0m     model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\serialization.py:1014\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1012\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1013\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1014\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _load(opened_zipfile,\n\u001b[0;32m   1015\u001b[0m                      map_location,\n\u001b[0;32m   1016\u001b[0m                      pickle_module,\n\u001b[0;32m   1017\u001b[0m                      overall_storage\u001b[38;5;241m=\u001b[39moverall_storage,\n\u001b[0;32m   1018\u001b[0m                      \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[0;32m   1019\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n\u001b[0;32m   1020\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmmap can only be used with files saved with \u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1021\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`torch.save(_use_new_zipfile_serialization=True), \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1022\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplease torch.save your checkpoint with this option in order to use mmap.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\serialization.py:1422\u001b[0m, in \u001b[0;36m_load\u001b[1;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1420\u001b[0m unpickler \u001b[38;5;241m=\u001b[39m UnpicklerWrapper(data_file, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[0;32m   1421\u001b[0m unpickler\u001b[38;5;241m.\u001b[39mpersistent_load \u001b[38;5;241m=\u001b[39m persistent_load\n\u001b[1;32m-> 1422\u001b[0m result \u001b[38;5;241m=\u001b[39m unpickler\u001b[38;5;241m.\u001b[39mload()\n\u001b[0;32m   1424\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[0;32m   1425\u001b[0m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_log_api_usage_metadata(\n\u001b[0;32m   1426\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.load.metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mserialization_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: zip_file\u001b[38;5;241m.\u001b[39mserialization_id()}\n\u001b[0;32m   1427\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\serialization.py:1415\u001b[0m, in \u001b[0;36m_load.<locals>.UnpicklerWrapper.find_class\u001b[1;34m(self, mod_name, name)\u001b[0m\n\u001b[0;32m   1413\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m   1414\u001b[0m mod_name \u001b[38;5;241m=\u001b[39m load_module_mapping\u001b[38;5;241m.\u001b[39mget(mod_name, mod_name)\n\u001b[1;32m-> 1415\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfind_class(mod_name, name)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pytorch_lightning'"
     ]
    }
   ],
   "source": [
    "from exercise_1.util.mlp.model import signed_distance_mlp\n",
    "\n",
    "# runs on cpu, so might be slow\n",
    "shape_grid = occupancy_grid(signed_distance_mlp, 63)\n",
    "visualize_occupancy(shape_grid, flip_axes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For those curious on how to train such a function, a minimal sample implementation is provided [here](https://github.com/nihalsid/shape_sdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. From Signed Distance Fields to Triangle Meshes\n",
    "\n",
    "Operating on regular voxel grids with either SDFs or Occupancy Grids has many advantages as you will also see in later exercises.\n",
    "\n",
    "However, eventually we want to be able to extract the shape encoded in an implicit representation into an explicit format that can be viewed more easily and is more widely supported by graphics pipelines. This is where polygon (or more specifially, triangle) meshes come into play.\n",
    "\n",
    "As presented in the lecture, the Marching Cubes [3] algorithm is one of the most widely used approaches to extract such meshes from implicit representations. In this task, we want to create a naive but fully functional implementation of this algorithm from scratch. You will find that it is surprisingly easy to implement!\n",
    "\n",
    "Marching Cubes iteratively loops over all cubes defined between the voxels of a regular grid.\n",
    "\n",
    "For each such cube, it does the following:\n",
    "- Compute an 8-bit index representing the specific configuration of its corners (storing for each corner whether it lies inside or outside of the shape)\n",
    "- Using this index, retreive information about where and how triangles intersect the cube edges from a pre-computed table\n",
    "- Add found vertices and faces to a global list\n",
    "\n",
    "We will use the following edge and corner indices for our implementation:\n",
    "\n",
    "<center><img src=\"exercise_1/images/mc_indices.png\" alt=\"Marching Cubes Indices\" style=\"width: 250px;\"/></center>\n",
    "\n",
    "This visualization also includes the axes specification we use for this exercise: Positive X to the right; Positive Y out of the image plane; Positive Z upwards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Compute the Cube Index\n",
    "\n",
    "Implement the function computing a cube index from the current cube by filling out function `compute_cube_index` in `exercise_1/marching_cubes.py`.\n",
    "\n",
    "The function takes a cube, represented by a list of SDF values from the corners of the cube. The order is given by the corner indices visualized above.\n",
    "\n",
    "For each corner, you should set the corresponding bit to 1 if that corner's SDF value is < 0 and to 1 otherwise.\n",
    "\n",
    "Example: A cube intersects the surface such that its top half lies within the shape (= top 4 corner voxels hold negative SDF values) and the bottom half outside (= bottom 4 corner voxels hold positive SDF values).\n",
    "\n",
    "Its index should then be 11110000 = 240.\n",
    "\n",
    "Note that corner 0 in your list of SDF values modifies the least significant bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cube Index for [-1 -1 -1 -1  1  1  1  1] = 00001111(=15)\n",
      "Cube Index for [ 1  1  1  1 -1 -1 -1 -1] = 11110000(=240)\n",
      "Cube Index for [1 1 1 1 1 1 1 1] = 00000000(=0)\n"
     ]
    }
   ],
   "source": [
    "from exercise_1.marching_cubes import compute_cube_index\n",
    "\n",
    "cube = np.array([-1, -1, -1, -1, 1, 1, 1, 1])\n",
    "index = compute_cube_index(cube)\n",
    "print(f\"Cube Index for {cube} = {index:08b}(={index})\")\n",
    "\n",
    "# This is the example from above\n",
    "cube = np.array([1, 1, 1, 1, -1, -1, -1, -1])\n",
    "index = compute_cube_index(cube)\n",
    "print(f\"Cube Index for {cube} = {index:08b}(={index})\")\n",
    "\n",
    "# Take a look at the triangle_table. How many triangles are defined for this index?\n",
    "cube = np.array([1, 1, 1, 1, 1, 1, 1, 1])\n",
    "index = compute_cube_index(cube)\n",
    "print(f\"Cube Index for {cube} = {index:08b}(={index})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Implement Marching Cubes\n",
    "Now that we have the index, all that is left to do is to loop over all possible cubes and to use the pre-defined triangle table that tells us which edges are intersected in the current cube. Indexing the triangle table with the cube index gives us between 0 and 5 edge index triples. We use each such triple to create a face. \n",
    "\n",
    "A (triangular) face is defined as a tuple of 3 integer indices with each index referring to a vertex in a corresponding vertex list. The minimal example in exercise part 1.4 gives an example for that: A face is defined in line 4 which references vertices 1, 2, and 3 defined in the vertex list in lines 1-3.\n",
    "\n",
    "In our case, we get the face vertices by placing a point on each edge that is referenced in our edge index triple. Use the pre-defined function `vertex_interpolation` to place that point halfway between the corners adjacent to the edge. Use the function as-is right now, we will modify it in part (c).\n",
    "\n",
    "Implement function `marching_cubes` in `exercise_1/marching_cubes.py` which takes an sdf grid as input and returns a tuple of 2 numpy arrays, the first for the vertex locations and the second for face definitions.\n",
    "\n",
    "You don't need to find and remove vertex duplicates for this naive implementation; this is something you would optimize when going for an ideal implementation, though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f968c750cd74acaa300548c39b478e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from exercise_1.marching_cubes import marching_cubes\n",
    "from exercise_1.util.visualization import visualize_mesh\n",
    "\n",
    "sdf_grid_sphere = sdf_grid(sdf_func_sphere, 32)\n",
    "vertices, faces = marching_cubes(sdf_grid_sphere)\n",
    "visualize_mesh(vertices, faces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Vertex Interpolation\n",
    "\n",
    "Your output should look like the left image in the visualization below.\n",
    "\n",
    "One thing we still have to take care of is to reduce the effect of the grid structure. In your implementation so far, the vertex locations were set to lie exactly half way between the two corners of an edge.\n",
    "\n",
    "One trick to reduce the amount of grid artifacts in our mesh is to interpolate the vertex locations based on their relative SDF values since each such value tells us precisely where the surface should lie.\n",
    "\n",
    "Change the implementation of function `vertex_interpolation` in file `exercise_1/marching_cubes.py` to interpolate the vertex location based on corner points p_1 and p_2 as well as their corresponding SDF values v_1 and v_2.\n",
    "\n",
    "Run your Marching Cubes implementation again with the modified function below. Your output should now look much smoother, like on the right side of the visualization below.\n",
    "\n",
    "No Interpolation | Interpolation\n",
    ":- | :-\n",
    "<img src=\"exercise_1/images/mc_no_interpolation.png\" alt=\"no interpolation\" style=\"width: 250px;\"/> | <img src=\"exercise_1/images/mc_interpolation.png\" alt=\"interpolation\" style=\"width: 250px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0260dcd9cc514f49b836c7935ec9ac98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vertices, faces = marching_cubes(sdf_grid_sphere)\n",
    "visualize_mesh(vertices, faces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also play around with your torus and atom shapes below.\n",
    "\n",
    "Try out the effects of different resolutions and how vertex interpolation affects things there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d11bd036475d459996587fe9a4c612a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sdf_grid_sphere = sdf_grid(sdf_func_sphere, 128)\n",
    "vertices, faces = marching_cubes(sdf_grid_sphere)\n",
    "visualize_mesh(vertices, faces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e57d046cf1946f8afa2cddc7fe574be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sdf_grid_torus = sdf_grid(sdf_func_torus, 64)\n",
    "vertices, faces = marching_cubes(sdf_grid_torus)\n",
    "visualize_mesh(vertices, faces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d39fdc075204d219e10b0ace2852698",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Note that the orbit of the atom is not preserved at 64^3 resolution or lower, try going higher in resolution, e.g. 128 (will be much slower)\n",
    "sdf_grid_atom = sdf_grid(signed_distance_atom, 64)\n",
    "vertices, faces = marching_cubes(sdf_grid_atom)\n",
    "visualize_mesh(vertices, faces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pytorch_lightning'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# runs on cpu, so might be slow\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m sdf_grid_mlp \u001b[38;5;241m=\u001b[39m sdf_grid(signed_distance_mlp, \u001b[38;5;241m64\u001b[39m)\n\u001b[0;32m      3\u001b[0m vertices, faces \u001b[38;5;241m=\u001b[39m marching_cubes(sdf_grid_mlp)\n\u001b[0;32m      4\u001b[0m visualize_mesh(vertices, faces, flip_axes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\Downloads\\tum-ml3d-excercises\\exercise1\\exercise_1\\sdf_grid.py:19\u001b[0m, in \u001b[0;36msdf_grid\u001b[1;34m(sdf_function, resolution)\u001b[0m\n\u001b[0;32m      5\u001b[0m def sdf_grid(sdf_function, resolution):\n\u001b[0;32m      6\u001b[0m     \"\"\"\n\u001b[0;32m      7\u001b[0m     Create an occupancy grid at the specified resolution given the implicit representation.\n\u001b[0;32m      8\u001b[0m     :param sdf_function: A function that takes in a point (x, y, z) and returns the sdf at the given point.\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m     :return: An SDF grid of specified resolution (i.e. an array of dim (resolution, resolution, resolution) with positive values outside the shape and negative values inside.\n\u001b[0;32m     13\u001b[0m     \"\"\"\n\u001b[0;32m     15\u001b[0m     # ###############\n\u001b[0;32m     16\u001b[0m     # TODO: Implement\n\u001b[0;32m     17\u001b[0m #     grid = np.zeros((resolution, resolution, resolution))\n\u001b[0;32m     18\u001b[0m #     x, y, z = np.mgrid[-1:1:resolution * 1j, -1:1:resolution * 1j, -1:1:resolution * 1j]\n\u001b[1;32m---> 19\u001b[0m #     grid = sdf_function(x, y, z)\n\u001b[0;32m     20\u001b[0m     grid = np.zeros((resolution, resolution, resolution))\n\u001b[0;32m     21\u001b[0m     step = 1.0 / (resolution - 1)\n",
      "File \u001b[1;32m~\\Downloads\\tum-ml3d-excercises\\exercise1\\exercise_1\\util\\mlp\\model.py:11\u001b[0m, in \u001b[0;36msigned_distance_mlp\u001b[1;34m(x, y, z)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msigned_distance_mlp\u001b[39m(x, y, z):\n\u001b[0;32m     10\u001b[0m     model \u001b[38;5;241m=\u001b[39m SimpleImplicitDecoder()\n\u001b[1;32m---> 11\u001b[0m     state_dict \u001b[38;5;241m=\u001b[39m rename_state_dict(torch\u001b[38;5;241m.\u001b[39mload((pathlib\u001b[38;5;241m.\u001b[39mPath(\u001b[38;5;18m__file__\u001b[39m)\u001b[38;5;241m.\u001b[39mparent \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheckpoint\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_ckpt_epoch=349.ckpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mabsolute(), map_location\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstate_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     12\u001b[0m     model\u001b[38;5;241m.\u001b[39mload_state_dict(state_dict)\n\u001b[0;32m     13\u001b[0m     model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\serialization.py:1014\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1012\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1013\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1014\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _load(opened_zipfile,\n\u001b[0;32m   1015\u001b[0m                      map_location,\n\u001b[0;32m   1016\u001b[0m                      pickle_module,\n\u001b[0;32m   1017\u001b[0m                      overall_storage\u001b[38;5;241m=\u001b[39moverall_storage,\n\u001b[0;32m   1018\u001b[0m                      \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[0;32m   1019\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n\u001b[0;32m   1020\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmmap can only be used with files saved with \u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1021\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`torch.save(_use_new_zipfile_serialization=True), \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1022\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplease torch.save your checkpoint with this option in order to use mmap.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\serialization.py:1422\u001b[0m, in \u001b[0;36m_load\u001b[1;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1420\u001b[0m unpickler \u001b[38;5;241m=\u001b[39m UnpicklerWrapper(data_file, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[0;32m   1421\u001b[0m unpickler\u001b[38;5;241m.\u001b[39mpersistent_load \u001b[38;5;241m=\u001b[39m persistent_load\n\u001b[1;32m-> 1422\u001b[0m result \u001b[38;5;241m=\u001b[39m unpickler\u001b[38;5;241m.\u001b[39mload()\n\u001b[0;32m   1424\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[0;32m   1425\u001b[0m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_log_api_usage_metadata(\n\u001b[0;32m   1426\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.load.metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mserialization_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: zip_file\u001b[38;5;241m.\u001b[39mserialization_id()}\n\u001b[0;32m   1427\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\serialization.py:1415\u001b[0m, in \u001b[0;36m_load.<locals>.UnpicklerWrapper.find_class\u001b[1;34m(self, mod_name, name)\u001b[0m\n\u001b[0;32m   1413\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m   1414\u001b[0m mod_name \u001b[38;5;241m=\u001b[39m load_module_mapping\u001b[38;5;241m.\u001b[39mget(mod_name, mod_name)\n\u001b[1;32m-> 1415\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfind_class(mod_name, name)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pytorch_lightning'"
     ]
    }
   ],
   "source": [
    "# runs on cpu, so might be slow\n",
    "sdf_grid_mlp = sdf_grid(signed_distance_mlp, 64)\n",
    "vertices, faces = marching_cubes(sdf_grid_mlp)\n",
    "visualize_mesh(vertices, faces, flip_axes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Writing Triangle Meshes to Disk as Wavefront OBJ\n",
    "\n",
    "Triangle meshes are simple data structures that can easily be stored in common data formats like OBJ.\n",
    "\n",
    "In this format, each line simply starts with a _v_ if it defines a vertex or _f_ if it defines a face.\n",
    "The letter is then followed by three float values (written as strings) denoting the x, y, and z locations for a vertex or three integer values denoting the indices of the vertices that are part of this triangle face. All values are separated by a space. Note that vertices are 1-indexed in the official obj specification.\n",
    "\n",
    "Minimal example:\n",
    "```\n",
    "v 1.0 2.0 3.0\n",
    "v 2.0 3.0 4.0\n",
    "v 3.0 4.0 5.0\n",
    "f 1 2 3\n",
    "```\n",
    "\n",
    "Implement the function `export_mesh_to_obj` in file `exercise_1/export.py` to write the list of vertices you got from Marching Cubes, together with the list of faces. The resulting file should be viewable in Meshlab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexercise_1\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexport\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m export_mesh_to_obj\n\u001b[0;32m      3\u001b[0m vertices, faces \u001b[38;5;241m=\u001b[39m marching_cubes(sdf_grid_sphere)\n\u001b[1;32m----> 4\u001b[0m export_mesh_to_obj(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexported_mesh.obj\u001b[39m\u001b[38;5;124m\"\u001b[39m, vertices, faces)\n",
      "File \u001b[1;32m~\\Downloads\\tum-ml3d-excercises\\exercise1\\exercise_1\\export.py:19\u001b[0m, in \u001b[0;36mexport_mesh_to_obj\u001b[1;34m(path, vertices, faces)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# write vertices starting with \"v \"\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# write faces starting with \"f \"\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# ###############\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# TODO: Implement\u001b[39;00m\n\u001b[0;32m     18\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x, y, z \u001b[38;5;129;01min\u001b[39;00m vertices:\n\u001b[0;32m     20\u001b[0m     f\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mv \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(x, y, z))\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m v_1, v_2, v_3 \u001b[38;5;129;01min\u001b[39;00m faces:     \n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 3)"
     ]
    }
   ],
   "source": [
    "from exercise_1.export import export_mesh_to_obj\n",
    "\n",
    "vertices, faces = marching_cubes(sdf_grid_sphere)\n",
    "export_mesh_to_obj(\"exported_mesh.obj\", vertices, faces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5. From Triangle Meshes to Point Clouds\n",
    "\n",
    "Point clouds are another 3D representation popular because of their simplicity and efficiency, and are quite often the output format provided by 3D capturing devices. In the lecture we discussed one way to sample point clouds from meshes by sampling uniformly across the mesh surface.\n",
    "\n",
    "This can be done by first determining the number of points we need to sample for each triangle, based on its surface area. Then, we sample that number of points using barycentric coordinates for each triangle by generating random numbers between 0 and 1 for r_1 and r_2 and then apply the formulas $$u=1-\\sqrt{r_1}, v=\\sqrt{r_1}(1-r_2), w=\\sqrt{r_1}r_2, P=uA+vB+wC$$ to get the coordinates of the sampled point **P**.\n",
    "\n",
    "Fill in the function `sample_point_cloud` in file `exercise_1/point_cloud.py` to realize such a sampling strategy.\n",
    "\n",
    "Hint: An easy way to sample based on surface area is to calculate the probability of sampling a triangle and then use `np.random.choice`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\zeyad.mahmoud\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3505, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\zeyad.mahmoud\\AppData\\Local\\Temp\\ipykernel_8816\\196485171.py\", line 7, in <module>\n",
      "    sampled_points = sample_point_cloud(vertices, faces, n_points)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\zeyad.mahmoud\\Downloads\\tum-ml3d-excercises\\exercise1\\exercise_1\\point_cloud.py\", line 51, in sample_point_cloud\n",
      "    v1 = vertices[v1_indices]\n",
      "         ~~~~~~~~^^^^^^^^^^^^\n",
      "IndexError: index 3 is out of bounds for axis 0 with size 1\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\zeyad.mahmoud\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2102, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\zeyad.mahmoud\\anaconda3\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1310, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\zeyad.mahmoud\\anaconda3\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1199, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\zeyad.mahmoud\\anaconda3\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1052, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\zeyad.mahmoud\\anaconda3\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 978, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\zeyad.mahmoud\\anaconda3\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 878, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "    ^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\zeyad.mahmoud\\anaconda3\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 712, in lines\n",
      "    return self._sd.lines\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\zeyad.mahmoud\\anaconda3\\Lib\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\zeyad.mahmoud\\anaconda3\\Lib\\site-packages\\stack_data\\core.py\", line 698, in lines\n",
      "    pieces = self.included_pieces\n",
      "             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\zeyad.mahmoud\\anaconda3\\Lib\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\zeyad.mahmoud\\anaconda3\\Lib\\site-packages\\stack_data\\core.py\", line 649, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\zeyad.mahmoud\\anaconda3\\Lib\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\zeyad.mahmoud\\anaconda3\\Lib\\site-packages\\stack_data\\core.py\", line 628, in executing_piece\n",
      "    return only(\n",
      "           ^^^^^\n",
      "  File \"C:\\Users\\zeyad.mahmoud\\anaconda3\\Lib\\site-packages\\executing\\executing.py\", line 164, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "from exercise_1.point_cloud import sample_point_cloud\n",
    "\n",
    "n_points = 5000\n",
    "\n",
    "sdf_grid_atom = sdf_grid(signed_distance_atom, 64)\n",
    "vertices, faces = marching_cubes(sdf_grid_atom)\n",
    "sampled_points = sample_point_cloud(vertices, faces, n_points)\n",
    "visualize_pointcloud(sampled_points, .25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MeshLab supports viewing point clouds stored as OBJ files. Since we don't have any faces, we can store only the vertices in the OBJ. \n",
    "\n",
    "Fill out the function `export_pointcloud_to_obj` in `exercise_1/export.py` and export the point cloud to disk. You can re-use parts of the export function you wrote before here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exercise_1.export import export_pointcloud_to_obj\n",
    "\n",
    "# export to \"exported_pointcloud.obj\", check out the mesh in MeshLab\n",
    "export_pointcloud_to_obj(\"exported_pointcloud.obj\", sampled_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6. Rigid Shape Aligment with Procrustes\n",
    "\n",
    "In the second lecture, you learned about shape alignment using Procrustes and Iterative Closest Point and shape description using various techniques.\n",
    "\n",
    "In this last part of the exercise, we will cover a simple method to align two shapes together: Solving the Orthogonal Procrustes problem [4]. If you want to learn more about shape description and alignment, we highly recommed taking the 3D Scanning & Motion Capture lecture as those topics are the focus of that lecture.\n",
    "\n",
    "Here we'll try to align the following two shapes, given near-perfect point correspondences between the shapes. Note that when working with real data, more involved methods like ICP are required since neither correspondences nor shape overlap will be perfect.\n",
    "\n",
    "Executing the cell below shows a visualization of the two shapes in their current state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "450dd805096d4d3195757781eb1ff6b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from exercise_1.util.visualization import visualize_shape_alignment\n",
    "visualize_shape_alignment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill out the function `procrustes_align` in `exercise_1/alignment.py` such that this function returns the rigid transformation from input (red) to target (green). It should return a rotation R and translation t which you can then use to visualize the final shape alignment using our pre-defined function `visualize_shape_alignment`.\n",
    "\n",
    "The function already takes in the correspondences, so you don't have to worry about that.\n",
    "\n",
    "For the given meshes and correspondences you should be able to get close to 0 aligment loss and almost perfect aligment of the meshes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exercise_1.alignment import procrustes_align, load_correspondences\n",
    "from exercise_1.util.visualization import visualize_shape_alignment\n",
    "\n",
    "# load correspondences\n",
    "correspondences_x, correspondences_y = load_correspondences()\n",
    "# align correspondences, in case of perfect aligment should be very close to zero (< 1e-8)\n",
    "R, t = procrustes_align(correspondences_x, correspondences_y)\n",
    "\n",
    "# perfect aligment would cause perfect superimposition of the two meshes\n",
    "# perfect aligment (on my system) appears as green mesh with gliching red colors when changing the viewing angle\n",
    "visualize_shape_alignment(R, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission\n",
    "\n",
    "This is the end of exercise 1 ðŸ™‚. Please create a zip containing all files we provided, everything you modified, and all of your generated output/visualization files. Name it with your matriculation number(s) as described on top. Make sure this notebook can be run without problems. Then, submit via Moodle.\n",
    "\n",
    "**Submission Deadline**: 22.11.2023 23:55"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] Park, Jeong Joon, et al. \"Deepsdf: Learning continuous signed distance functions for shape representation.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019\n",
    "\n",
    "[2] Mescheder, Lars, et al. \"Occupancy networks: Learning 3d reconstruction in function space.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019.\n",
    "\n",
    "[3] Lorensen, William E., and Harvey E. Cline. \"Marching cubes: A high resolution 3D surface construction algorithm.\" ACM siggraph computer graphics 21.4 (1987): 163-169.\n",
    "\n",
    "[4] SchÃ¶nemann, Peter H. \"A generalized solution of the orthogonal procrustes problem.\" Psychometrika 31.1 (1966): 1-10.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
